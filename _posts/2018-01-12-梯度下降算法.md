---
layout: post
title: "梯度下降算法"
date: 2018-01-12
description: "梯度下降算法"
tag: 机器学习
---
### 梯度下降算法
梯度下降算法是一个用来求函数最小值的算法，我们将使用梯度下降算法来求出代价函数$$J(\theta_0,\theta_1)$$的最小值。

我们想要使用梯度下降来找到最优的$$\theta_0,\theta_1$$：首先随机选择两个$$\theta_0,\theta_1$$(例如，$$\theta_0=0,\theta_1=0$$)，不断地改变它们的值使得$$J(\theta)$$变小，最终找到`$J(\theta)$`的最小值点。此时我们得到的最小值为局部最小值，梯度下降算法受初始状态的影响，所以不能确定我们得到的局部最小值是否为全局最小值，选择不同的初始参数组合，可能会找到不同的局部最小值。

![](/images/2018-1-12/1.png)

批量梯度下降算法公式:


repeat until convergence{\n
$$
\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)
$$\n
}

+ := -表示赋值
+ $$\alpha$$ -学习率，它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大。α越小，步长越小，α越大，步长越大
+ $$\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)$$-梯度

在批量梯度下降中，我们每一次都同时让所有的参数减去学习速率乘以代价函数的导数。

在梯度下降算法中，我们要更新$$\theta_0$$和$$\theta_1$$，当`$j=0$`和`$j=1$`时，会产生更新，所以你需要同时更新$$\theta_0$$和$$\theta_1$$。

实现方法：计算公式右边部分，通过那一部分计算出$$\theta_0$$和$$\theta_1$$的值，然后同时更新$$\theta_0$$和$$\theta_1$$。

$$
temp0:=\theta_0-\alpha\frac{\partial}{\partial\theta_0}J(\theta_0,\theta_1)
$$
$$
temp1:=\theta_1-\alpha\frac{\partial}{\partial\theta_1}J(\theta_0,\theta_1)
$$
$$
\theta_0:=temp0
$$
$$
\theta_1:=temp1
$$

梯度和学习速率

$$
\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta)
$$
对$$\theta$$赋值，使得$$J(\theta)$$按梯度下降最快的方向进行，一直迭代下去，最终得到局部最小值。$$\alpha$$是学习率，它决定了我们沿着能让代价函数下降程度最大的方
向向下迈出的步子有多大。

如果$$\alpha$$太小了，即我的学习速率太小，结果就是只能这样像小宝宝一样一点点地挪动，去努力接近最低点，这样就需要很多步才能到达最低点，所以如果$$\alpha$$太小的话，可能会很慢,因为它会一点点挪动，它会需要很多步才能到达全局最低点。

如果$$\alpha$$太大，那么梯度下降法可能会越过最低点，甚至可能无法收敛，下一次迭代又移动了一大步，越过一次，又越过一次，一次次越过最低点，直到你发现实际上离最低点越来越远，所以，如果$$\alpha$$太大，它会导致无法收敛，甚至发散。

由于随着我们越来越接近最低点，相应的梯度（绝对值）也会逐渐减小，所以每次下降程度就会越来越小，我们并不需要减小$$\alpha$$的值来减小下降程度。

### 梯度下降的线性回归

梯度下降算法：
$$
\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)
$$
$$
(for j = 1 and j = 0)
$$
线性回归算法:
$$
h_\theta(x)=\theta_0+\theta_1x
$$
$$
J(\theta)=\frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2
$$
在线性回归问题运用梯度下降法，关键在于求出代价函数的导数：
$$
\frac{\partial}{\partial\theta_j}J(\theta_0\theta_1)=\frac{\partial}{\partial\theta_j}\frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2
$$
j=0时：
$$
\frac{\partial}{\partial\theta_0}J(\theta_0\theta_1)=\frac{1}{m}\sum_{i=1}{m}(h_\theta(x^{(i)})-y{(i)})
$$
j=1时:
$$
\frac{\partial}{\partial\theta_1}J(\theta_0\theta_1)=\frac{1}{m}\sum_{i=1}^{m}((h_\theta(x^{(i)})-y{(i)}*x^{(i)})
$$
则算法改写成：
$$
\theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})
$$
$$
\theta_1:=\theta_1-\alpha\frac{1}{m}\sum_{i=1}^{m}((h_\theta(x^{(i)}))
$$

批量梯度下降：在梯度下降的每一步中，我们都用到了
所有的训练样本，在梯度下降中，在计算微分求导项时，我们需要进行求和运算，所以，在
每一个单独的梯度下降中，我们最终都要计算这样一个东西，这个项需要对所有 m 个训练
样本求和。